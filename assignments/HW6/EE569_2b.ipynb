{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EE569_2b.ipynb","provenance":[],"authorship_tag":"ABX9TyN3Wc34P/5ocKAEWOmJn8OY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"25nCL8n3KQEw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651436089764,"user_tz":420,"elapsed":881583,"user":{"displayName":"Hanyun Zhao","userId":"16373381763566944252"}},"outputId":"affd0d5b-8475-450c-e3fe-f1053192bede"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n","=============================================>c/w Saab Train Hop 1\n","=============================================>c/w Saab Train Hop 2\n","=============================================>c/w Saab Train Hop 3\n","train accuracy 0.9593\n","test accuracy 0.9059\n","K1 1 K2 74 K3 47\n","total number of parameter 3050\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","import sys\n","sys.path.append('/content/drive/MyDrive/HW6')\n","\n","import numpy as np\n","from tensorflow.keras.datasets import mnist,fashion_mnist\n","from skimage.util import view_as_windows\n","from pixelhop import Pixelhop\n","from skimage.measure import block_reduce\n","import xgboost as xgb\n","import warnings, gc\n","import time\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","\n","\n","\n","np.random.seed(1)\n","\n","# Preprocess\n","N_Train_Reduced = 10000    # 10000\n","N_Train_Full = 60000     # 50000\n","N_Test = 10000            # 10000\n","\n","BS = 2000 # batch size\n","\n","\n","def shuffle_data(X, y):\n","    shuffle_idx = np.random.permutation(y.size)\n","    X = X[shuffle_idx]\n","    y = y[shuffle_idx]\n","    return X, y\n","\n","\n","def select_balanced_subset(images, labels, use_num_images):\n","    '''\n","    select equal number of images from each classes\n","    '''\n","    num_total, H, W, C = images.shape\n","    num_class = np.unique(labels).size\n","    num_per_class = int(use_num_images / num_class)\n","\n","    # Shuffle\n","    images, labels = shuffle_data(images, labels)\n","\n","    selected_images = np.zeros((use_num_images, H, W, C))\n","    selected_labels = np.zeros(use_num_images)\n","\n","    for i in range(num_class):\n","        selected_images[i * num_per_class:(i + 1) * num_per_class] = images[labels == i][:num_per_class]\n","        selected_labels[i * num_per_class:(i + 1) * num_per_class] = np.ones((num_per_class)) * i\n","\n","    # Shuffle again\n","    selected_images, selected_labels = shuffle_data(selected_images, selected_labels)\n","\n","    return selected_images, selected_labels\n","\n","def Shrink(X, shrinkArg):\n","    #---- max pooling---- (2x2) -to- (1x1)\n","    pool = shrinkArg['pool']\n","    # TODO: fill in the rest of max pooling\n","    if pool:\n","        N, H, W, C = X.shape\n","        if H%2!=0 or W%2!=0:\n","          print(\"non even height or width\")\n","        pool_out=np.zeros((N,int(H/2),int(W/2),C),dtype=int)\n","        for n in range(N):\n","            for c in range(C):\n","                for h in range(int(H/2)):\n","                    for w in range(int(W/2)):\n","                        pool_out[n,h,w,c]=X[n,h*2:h*2+1,w*2:w*2+1,c].max()\n","    else:\n","        pool_out=X\n","\n","    #---- neighborhood construction\n","    win = shrinkArg['win']\n","    stride = shrinkArg['stride']\n","    pad = shrinkArg['pad']\n","    ch=pool_out.shape[-1]\n","    # TODO: fill in the rest of neighborhood construction\n","\n","    if pad>0:\n","        out=np.pad(pool_out,((0,0), (pad,pad), (pad,pad), (0,0)), 'reflect')\n","    else:\n","        out=pool_out\n","    out = view_as_windows(out, (1, win, win, ch), (1, stride, stride, ch))\n","    return out.reshape(out.shape[0], out.shape[1], out.shape[2], -1)\n","\n","\n","# example callback function for how to concate features from different hops\n","def Concat(X, concatArg):\n","    return X\n","\n","def get_feat(X, num_layers=3):\n","    output = p2.transform_singleHop(X,layer=0)\n","    if num_layers>1:\n","        for i in range(num_layers-1):\n","            output = p2.transform_singleHop(output, layer=i+1)\n","    return output\n","\n","\n","\n","warnings.filterwarnings(\"ignore\")\n","# ---------- Load MNIST data and split ----------\n","(x_train, y_train), (x_test,y_test) = mnist.load_data()\n","\n","\n","# -----------Data Preprocessing-----------\n","x_train = np.asarray(x_train,dtype='float32')[:,:,:,np.newaxis]\n","x_test = np.asarray(x_test,dtype='float32')[:,:,:,np.newaxis]\n","y_train = np.asarray(y_train,dtype='int')\n","y_test = np.asarray(y_test,dtype='int')\n","\n","# if use only 10000 images train pixelhop\n","x_train_reduced, y_train_reduced = select_balanced_subset(x_train, y_train, use_num_images=N_Train_Reduced)\n","\n","x_train /= 255.0\n","x_test /= 255.0\n","x_train_reduced /=255.0\n","\n","#TODO:\n","\n","# train_accuracy=[]\n","# test_accuracy=[]\n","# modelSize=[]\n","\n","# -----------Module 1: set PixelHop parameters-----------\n","# TODO: fill in this part\n","SaabArgs = [{'num_AC_kernels':-1, 'needBias':False, 'cw': False},\n","            {'num_AC_kernels':-1, 'needBias':True, 'cw':False},\n","            {'num_AC_kernels':-1, 'needBias':True, 'cw':False}]\n","shrinkArgs = [{'func':Shrink, 'win':5, 'stride': 1,'pad':2,'pool':False},\n","            {'func': Shrink, 'win':5, 'stride': 1,'pad':0,'pool':True},\n","            {'func': Shrink, 'win':5, 'stride': 1,'pad':0,'pool':True}]\n","concatArg = {'func':Concat}\n","\n","# -----------Module 1: Train PixelHop -----------\n","# TODO: fill in this part\n","p2 = Pixelhop(depth=3, TH1=0.005, TH2=0.001,\n","            SaabArgs=SaabArgs, shrinkArgs=shrinkArgs, concatArg=concatArg)\n","p2.fit(x_train_reduced)\n","\n","# --------- Module 2: get only Hop 3 feature for both training set and testing set -----------\n","# you can get feature \"batch wise\" and concatenate them if your memory is restricted\n","# TODO: fill in this part\n","x_train_feature=[]\n","x_test_feature=[]\n","for i in range(5):\n","  train_hop3_feats = get_feat(x_train_reduced[2000*i:2000*i+2000]) #get_feat is already set to get 3rd feat\n","  x_train_feature.append(train_hop3_feats)\n","  test_hop3_feats = get_feat(x_test[2000*i:2000*i+2000])\n","  x_test_feature.append(test_hop3_feats)\n","\n","train_hop3_feats=np.concatenate((x_train_feature[0],x_train_feature[1],x_train_feature[2],x_train_feature[3],x_train_feature[4]),axis=0)\n","test_hop3_feats=np.concatenate((x_test_feature[0],x_test_feature[1],x_test_feature[2],x_test_feature[3],x_test_feature[4]),axis=0)\n","\n","#test_hop3_feats = get_feat(x_test)\n","\n","\n","# --------- Module 2: standardization\n","STD = np.std(train_hop3_feats, axis=0, keepdims=1)\n","train_hop3_feats = train_hop3_feats/STD\n","test_hop3_feats = test_hop3_feats/STD\n","\n","train_hop3_feats = np.reshape(train_hop3_feats,(10000,train_hop3_feats.shape[3]))\n","test_hop3_feats = np.reshape(test_hop3_feats,(10000,test_hop3_feats.shape[3]))\n","\n","#---------- Module 3: Train XGBoost classifier on hop3 feature ---------\n","\n","tr_acc = []\n","te_acc = []\n","\n","clf = xgb.XGBClassifier(n_jobs=-1,\n","                    objective='multi:softprob',\n","                    # tree_method='gpu_hist', gpu_id=None,\n","                    max_depth=6,n_estimators=100,\n","                    min_child_weight=5,gamma=5,\n","                    subsample=0.8,learning_rate=0.1,\n","                    nthread=8,colsample_bytree=1.0)\n","\n","# train_hop3_feats = np.reshape(train_hop3_feats,(10000,train_hop3_feats.shape[3]))\n","\n","# test_hop3_feats = np.reshape(test_hop3_feats,(10000,test_hop3_feats.shape[3]))\n","clf.fit(train_hop3_feats, y_train_reduced)\n","\n","train_pred = clf.predict(train_hop3_feats)\n","test_pred = clf.predict(test_hop3_feats)\n","\n","train_acc = accuracy_score(y_train_reduced, train_pred)\n","test_acc = accuracy_score(y_test, test_pred)\n","print(\"train accuracy\",train_acc)\n","print(\"test accuracy\",test_acc)\n","\n","\n","K1=len(p2.Energy[\"Layer0\"])\n","K2=sum([len(i) for i in p2.Energy[\"Layer1\"]])\n","K3=sum([len(i) for i in p2.Energy[\"Layer2\"]])\n","print(\"K1\",K1,\"K2\",K2,\"K3\",K3)\n","print(\"total number of parameter\", 25*K1+25*K1*K2+25*K2*K3)\n"]},{"cell_type":"code","source":["print(test_pred)"],"metadata":{"id":"HwDsxNJZQ2X8","executionInfo":{"status":"error","timestamp":1651452907823,"user_tz":420,"elapsed":179,"user":{"displayName":"Hanyun Zhao","userId":"16373381763566944252"}},"outputId":"a74f0657-cadb-4b4e-ebf9-66dbe8689e14","colab":{"base_uri":"https://localhost:8080/","height":175}},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-74788271785b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'test_pred' is not defined"]}]}]}